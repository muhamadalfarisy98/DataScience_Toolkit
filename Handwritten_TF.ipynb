{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Handwritten_TF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxbYeKPpbgJl",
        "outputId": "706fb9e7-0888-4470-870d-f951288ff0bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        }
      },
      "source": [
        "pip install tensorflow==1.4.0"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9f/be0165c6eefd841e6928e54d3d083fa174f92d640fdc52f73a33dc9c54d1/tensorflow-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (41.2MB)\n",
            "\u001b[K     |████████████████████████████████| 41.2MB 92kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4.0) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4.0) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4.0) (0.35.1)\n",
            "Collecting enum34>=1.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/63/f6/ccb1c83687756aeabbf3ca0f213508fcfb03883ff200d201b3a4c60cedcc/enum34-1.1.10-py3-none-any.whl\n",
            "Collecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/9f/5845c18f9df5e7ea638ecf3a272238f0e7671e454faa396b5188c6e6fc0a/tensorflow_tensorboard-0.4.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 39.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.3.0->tensorflow==1.4.0) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (3.2.2)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 37.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (3.2.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=4fb59947694456dad7445a83160bf4d27aa7e068f3b7ef91791ea906f02a2cd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: enum34, html5lib, bleach, tensorflow-tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.2.1\n",
            "    Uninstalling bleach-3.2.1:\n",
            "      Successfully uninstalled bleach-3.2.1\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed bleach-1.5.0 enum34-1.1.10 html5lib-0.9999999 tensorflow-1.4.0 tensorflow-tensorboard-0.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L1XVGQohNfa",
        "outputId": "514f5276-fa88-4898-c7c0-6242ef9b9a57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "pip install image==1.5.20\n",
        "pip install numpy==1.14.3"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-d6a06d51d2ef>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install image==1.5.20\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nnzEKoPS64o",
        "outputId": "5a485f60-c963-4402-9c9c-06953a9da8fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "#Using TensorFlow to recognize HandWritten Digits from MNIST dataset\n",
        "#this dataset is made up of images of handwritten digits (28x28 pixels in size).\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
            "  return f(*args, **kwds)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DedCtkkeToDV",
        "outputId": "670500b6-f043-4192-919f-8ba57f478942",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "print(tensorflow)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-aec32c7cda98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tensorflow' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZXJgc0TXRSG",
        "outputId": "b055842d-8037-468d-f9bb-efd928a93f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# import tensorflow as tf\n",
        "# (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cahP6QysXndL"
      },
      "source": [
        "# mnist = tf.keras.datasets.mnist\n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jLBed8oThNI"
      },
      "source": [
        "# DATA_URL = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'\n",
        "\n",
        "# path = tf.keras.utils.get_file('mnist.npz', DATA_URL)\n",
        "# with np.load(path) as data:\n",
        "#   train_examples = data['x_train']\n",
        "#   train_labels = data['y_train']\n",
        "#   test_examples = data['x_test']\n",
        "#   test_labels = data['y_test']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSHW-eRgTUqM"
      },
      "source": [
        "#import MNIST dataset \n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF_-tM0bcIjp",
        "outputId": "9ae37744-90f0-424e-ac9b-5434ffea82ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
        "#labels are oh-encoded"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSNP2F_AcWyJ"
      },
      "source": [
        "n_train=mnist.train.num_examples #55000\n",
        "n_validation=mnist.validation.num_examples #5000\n",
        "n_test=mnist.test.num_examples #10000"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyBk_CTgXwjb"
      },
      "source": [
        "#Defining the Neural Network Architecture\n",
        "n_input=784\n",
        "n_hidden1=512\n",
        "n_hidden2=256\n",
        "n_hidden3=128\n",
        "n_output=10"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJKvLUZuYENU"
      },
      "source": [
        "#hyperparameters\n",
        "learning_rate=1e-4\n",
        "n_iteration=1000\n",
        "batch_size=128\n",
        "dropout=0.5"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UyaNW67ZEJd"
      },
      "source": [
        "#setu up network as computational graph for TS to execute\n",
        "#the core concept of tensorFlow is the tensor. data structure that similar to an array or list.\n",
        "\n",
        "#defining 3 tensors as placeholders, which are tensor that we''ll feed values into later.\n",
        "X=tf.placeholder(\"float\",[None,n_input])\n",
        "Y=tf.placeholder(\"float\",[None,n_output])\n",
        "keep_prob=tf.placeholder(tf.float32)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufGt8v8FbxPM"
      },
      "source": [
        "#the parameter that the network will update in training process are the weight and bias values.\n",
        "#so for these we need to set an initial value rather than an empty placeholder.\n",
        "# we''ll be using random values from a truncated normal distribution for the weights.\n",
        "\n",
        "weights= {'w1':tf.Variable(tf.truncated_normal([n_input,n_hidden1],stddev=0.1)),\n",
        "          'w2':tf.Variable(tf.truncated_normal([n_hidden1,n_hidden2],stddev=0.1)),\n",
        "          'w3':tf.Variable(tf.truncated_normal([n_hidden2,n_hidden3],stddev=0.1)),\n",
        "          'out':tf.Variable(tf.truncated_normal([n_hidden3,n_output],stddev=0.1)),\n",
        "         }\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md4RGfHNhGtQ"
      },
      "source": [
        "#for bias we use a small constant value to ensure that the tensors activate in the initial stages and therefore contribute to the propagation\n",
        "biases ={\n",
        "        'b1':tf.Variable(tf.constant(0.1,shape=[n_hidden1])),\n",
        "         'b2':tf.Variable(tf.constant(0.1,shape=[n_hidden2])),\n",
        "         'b3':tf.Variable(tf.constant(0.1,shape=[n_hidden3])),\n",
        "         'out':tf.Variable(tf.constant(0.1,shape=[n_output])),\n",
        "\n",
        "        }"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CmfqbRaiK4Q"
      },
      "source": [
        "#next setup the layers of the network by defining the operations that will manipulate the tensor\n",
        "\n",
        "layer_1=tf.add(tf.matmul(X,weights['w1']),biases['b1'])\n",
        "layer_2=tf.add(tf.matmul(layer_1,weights['w2']),biases['b2'])\n",
        "layer_3=tf.add(tf.matmul(layer_2,weights['w3']),biases['b3'])\n",
        "layer_drop=tf.nn.dropout(layer_3,keep_prob)\n",
        "output_layer=tf.matmul(layer_3,weights['out']) +biases['out']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR8L1mUbi7ZO"
      },
      "source": [
        "#each hidden layer will execute matrix mul on the previous layer's output and the current layer's weights and add the bias to these values. at the last hidden layer,\n",
        "#we will apply a dropout opreation using our keep_prob value of 0.5\n",
        "#the final step in building the graph is to define the loss function \n",
        "\n",
        "\n",
        "#the popular choise is cross-entropy or log-loss\n",
        "#and need to choose the optimization algo which will be used to minimize the loss function. \n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIwtfEkUovao"
      },
      "source": [
        "# a process named gradient descent is a common method for finding minimium local of a function\n",
        "#by taking iterative steps along the gradient in a negative direction.\n",
        "#in this using Adam Optimizer. by using memntum to speed up the process through computing an exponentially weighted avearage of the gradient\n",
        "cross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=output_layer))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTtTfvkOpPsX"
      },
      "source": [
        "train_step=tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5INciyD4pYM-"
      },
      "source": [
        "#next step is feed data through the graph to train it and then test it "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UOr-TMWpfM-"
      },
      "source": [
        "#Train and Testing\n",
        "#involoves feeding the training dataset through the graph and optimizing the loss function.\n",
        "#everytime the network iterates through a batch of more training . it updates the parameters to reduce the loss\n",
        "#in order to more accurately predict the digits shown\n",
        "#the testing involves running testing dataset through trained graph and keepong track of the number\n",
        "#so that we can calculate the accuracy\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y79T8Qb4pnQ9"
      },
      "source": [
        "#defining our method of evaluating the accuracy\n",
        "correct_pred=tf.equal(tf.argmax(output_layer,1),tf.argmax(Y,1))\n",
        "accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3psS1lWjqaMm"
      },
      "source": [
        "#feed the same graph with tnew test examples\n",
        "init=tf.global_variables_initializer()\n",
        "sess=tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5ztPmSqqj62"
      },
      "source": [
        "#the essence of the training process in deep learning is to optimize the loss function\n",
        "#here we are aiming to minimze the diff between the predict labels and true labels\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-7BEtp0q8Gk",
        "outputId": "eb3dba69-9655-42a0-e1ca-25bdd9467e9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "#train on mini batches\n",
        "for i in range(n_iteration):\n",
        "    batch_x,batch_y=mnist.train.next_batch(batch_size)\n",
        "    sess.run(train_step,feed_dict={X:batch_x,Y:batch_y,keep_prob:dropout})\n",
        "    if i % 100 ==0:\n",
        "        minibatch_loss,minibatch_accuracy=sess.run([cross_entropy,accuracy],\n",
        "                                                   feed_dict={X:batch_x,Y:batch_y,keep_prob:1.0}\n",
        "        )\n",
        "        print(\"Iteration\",str(i)\n",
        "        ,\"\\t| Loss=\", str(minibatch_loss),\n",
        "        \"\\t| Accuracy =\",\n",
        "        str(minibatch_accuracy))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 \t| Loss= 3.3631082 \t| Accuracy = 0.109375\n",
            "Iteration 100 \t| Loss= 0.54438007 \t| Accuracy = 0.8125\n",
            "Iteration 200 \t| Loss= 0.32566863 \t| Accuracy = 0.890625\n",
            "Iteration 300 \t| Loss= 0.38131022 \t| Accuracy = 0.8984375\n",
            "Iteration 400 \t| Loss= 0.46649036 \t| Accuracy = 0.8828125\n",
            "Iteration 500 \t| Loss= 0.38832074 \t| Accuracy = 0.8671875\n",
            "Iteration 600 \t| Loss= 0.36833513 \t| Accuracy = 0.875\n",
            "Iteration 700 \t| Loss= 0.268458 \t| Accuracy = 0.9375\n",
            "Iteration 800 \t| Loss= 0.2693613 \t| Accuracy = 0.9296875\n",
            "Iteration 900 \t| Loss= 0.39138532 \t| Accuracy = 0.9296875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFw2Hd0srr0l",
        "outputId": "7aaf2240-b388-475d-ee73-39f9d163c973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "test_accuracy=sess.run(accuracy,feed_dict={X:mnist.test.images,Y:mnist.test.labels,keep_prob:1.0}\n",
        ")\n",
        "print(\"\\nAccuracy on test set:\",test_accuracy)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on test set: 0.915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmVmghYPsFzN"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUIbL7gEsWLb"
      },
      "source": [
        "# img=np.invert(Image.open(\"test_img.png\").convert('L')).ravel()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1elu3fBUt7Df"
      },
      "source": [
        "# prediction=sess.run(tf.argmax(output_layer,1),feed_dict={X:[img]})\n",
        "# print(np.squeeze(prediction))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOVIF905shKd",
        "outputId": "e14e9919-41b7-433b-b592-14a188184917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "curl -o images/test_img.png"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-a9be66c0435a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    curl -o images/test_img.png\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am4X3k0Psvti"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}